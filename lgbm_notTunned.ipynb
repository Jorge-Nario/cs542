{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport time\nfrom itertools import chain\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport lightgbm as lgb\n\nfrom kaggle.competitions import twosigmanews\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\n\n(market_train_df, news_train_df) = env.get_training_data()\n(market_train_df.shape, news_train_df.shape)\n\n#trim to managable dataset\nmarket_train_df = market_train_df.tail(3_000_000)\nnews_train_df = news_train_df.tail(6_000_000)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b43dc7eca58cbed33d66de75ff819f068dbc18f0"
      },
      "cell_type": "code",
      "source": "market_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba40bc965e957d21fc242239bd7278f43f7d1758"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b562e95fc3923775e44e64a57b2c10393b5fb25a"
      },
      "cell_type": "code",
      "source": "cols = list(news_train_df.columns.values)\n\nnews_cols_agg = {key:['min', 'max', 'mean', 'std'] for  key in cols}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e347512ff3c92e463ddcde299d0c957b9b8fac6f"
      },
      "cell_type": "code",
      "source": "news_cols_agg = {\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std'],\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std']\n}\n\nnews_train_df['assetCodes'] ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d4071fdbe89a49cf29090d0e2952723a4bae822"
      },
      "cell_type": "code",
      "source": "def merge_data(market_train_df, news_train_df):\n\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n    \n    #unpacks the dictionary into an expended list form\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    \n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df\n    del df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory \n    del news_train_df_expanded\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n    \n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "178874767db8f8a8dab103e47d4a2cc7c7eb068f"
      },
      "cell_type": "code",
      "source": "def get_returns(market_train_df, news_train_df, lable_encoder=None):\n    x, lable_encoder = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y, lable_encoder\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    lable_encoder = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return lable_encoder\n\ndef get_x(market_train_df, news_train_df, label_encoder=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    x = merge_data(market_train_df, news_train_df)\n    \n    if label_encoder is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        le_assetCode, le_assetName = label_encoder\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    \n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n\n    # Fix some mixed-type columns\n    for tmp in ['marketCommentary_min', 'marketCommentary_max']:\n        x[tmp] = x[tmp].astype(float)\n    \n    del tmp\n    #return the lable encoder as a tuple\n    return x, (le_assetCode, le_assetName)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "daac10eb661bb1ba0447863c073cc87074634b79"
      },
      "cell_type": "code",
      "source": "X, y, lable_encoder = get_returns(market_train_df, news_train_df)\nX.shape, y.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cbddedaff4bdc355c890c3b209351b6fead16db7"
      },
      "cell_type": "code",
      "source": "universe = market_train_df['universe']\ntime = market_train_df['time']\n\n# Free memory\ndel market_train_df, news_train_df\n\nprint(X.shape, y.shape)\n\nX.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b066cc33d71e977fd784edf34061d936f8bf482"
      },
      "cell_type": "code",
      "source": "#80 percent for training\ntraining_set = int(X.shape[0] * 0.8)\n\nX_train, y_train = X.iloc[:training_set], y.iloc[:training_set]\nX_valid, y_valid = X.iloc[training_set:], y.iloc[training_set:]\n\n# keep only those in the universe as rules\nuniv = (universe.iloc[training_set:] > 0)\nvalid_time = time.iloc[training_set:]\n\nX_valid = X_valid[univ]\ny_valid = y_valid[univ]\nvalid_time = valid_time[univ]\ndel univ\n\n# Creat lgb datasets\ntrain_cols = X.columns.tolist()\ncategorical_cols = [] \n\ntrain_dataset = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\nvalid_dataset = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n\nvalid_dataset.params = {\n    'extra_time': valid_time.factorize()[0]\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc5835f37d521514f997dd54abf2f135e2d9c59e"
      },
      "cell_type": "markdown",
      "source": "#  Build Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ab05940ae14206c29f982d9bafdd6c76b856293"
      },
      "cell_type": "code",
      "source": "lgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 127,\n    max_depth = -1,\n    bagging_fraction = 0.75,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    seed = 71,\n    metric = 'None'\n)\n\n'''\nCustom Scoring metric\n\n@predictions = np array of predictions\n@valid_dataset = dataset to do validation testing\n'''\ndef sig_score(predictions, valid_dataset):\n    df_time = valid_dataset.params['extra_time']\n    labels = valid_dataset.get_label()\n    results = predictions * labels\n    results_sum = results.groupby(df_time).sum()\n    score = results_sum.mean() / results_sum.std()\n    return 'sigma_score', score, True\n\nevals_result = {}\nmodel = lgb.train(lgb_params, train_dataset, num_boost_round=1000, valid_sets=(valid_dataset,), valid_names=('valid',), verbose_eval=25,\n              early_stopping_rounds=100, feval=sig_score, evals_result=evals_result)\n\ndf_result = pd.DataFrame(evals_result['valid'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae871957ebfc2dd66472c342f5982e9419a73ec6"
      },
      "cell_type": "markdown",
      "source": "# Plot the Results\n\n### Its learning rather quickly and then plateus "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c433fe9f83c6188669c7300828e51cfad8d2c09f"
      },
      "cell_type": "code",
      "source": "plot = df_result.plot(figsize=(12, 8))\nplot.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')\nprint(plot)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c9f7765d7ae89ac57cffdfaffdbead4285bc843"
      },
      "cell_type": "code",
      "source": "num_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\nprint(lgb_params)\nprint(f'Best score was {valid_score:.5f} on round {num_boost_round}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c728f119b63272bc2b284350e3c475021bf5950"
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(1, 2, figsize=(14, 14))\nlgb.plot_importance(model, ax=ax[0])\nlgb.plot_importance(model, ax=ax[1], importance_type='gain')\nfig.tight_layout()\n\n#clear space\ndel model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41ac7947f861b0f21e860f398a787ca2b58d0324"
      },
      "cell_type": "code",
      "source": "# Train full model\n# train_dataset = lgb.Dataset(X, y, feature_name=train_cols, categorical_feature=categorical_cols)\n\nmodel = lgb.train(lgb_params, train_dataset, num_boost_round=num_boost_round)\n\n#use model for predictions\ndef make_predictions(predictions_template_df, market_obs_df, news_obs_df, label_encoder):\n    x = get_x(market_obs_df, news_obs_df, lable_encoder)[0]\n    predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)\n\n    \ndays = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df, lable_encoder)\n    env.predict(predictions_template_df)\n\nenv.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6a5d07f3fb84cace97339334f7997964432f2c1"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}